{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Agustinaginting/Final-Exam-Komputasi-Lanjut/blob/main/Explainable_Matrix_Factorization_(EMF).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7IaJ87Wqt-0"
      },
      "source": [
        "Download Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihiaLpQAqocY",
        "outputId": "9cbbb110-362c-436f-c3e5-2d806e718668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-03 03:27:15--  https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip [following]\n",
            "--2023-01-03 03:27:15--  https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15312323 (15M) [application/zip]\n",
            "Saving to: ‘recsys.zip’\n",
            "\n",
            "recsys.zip          100%[===================>]  14.60M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-01-03 03:27:15 (127 MB/s) - ‘recsys.zip’ saved [15312323/15312323]\n",
            "\n",
            "Archive:  recsys.zip\n",
            "   creating: recsys/\n",
            "  inflating: recsys/datasets.py      \n",
            "  inflating: recsys/preprocessing.py  \n",
            "  inflating: recsys/utils.py         \n",
            "  inflating: recsys/requirements.txt  \n",
            "   creating: recsys/.vscode/\n",
            "  inflating: recsys/.vscode/settings.json  \n",
            "   creating: recsys/__pycache__/\n",
            "  inflating: recsys/__pycache__/datasets.cpython-36.pyc  \n",
            "  inflating: recsys/__pycache__/datasets.cpython-37.pyc  \n",
            "  inflating: recsys/__pycache__/utils.cpython-36.pyc  \n",
            "  inflating: recsys/__pycache__/preprocessing.cpython-37.pyc  \n",
            "  inflating: recsys/__pycache__/datasets.cpython-38.pyc  \n",
            "  inflating: recsys/__pycache__/preprocessing.cpython-36.pyc  \n",
            "  inflating: recsys/__pycache__/preprocessing.cpython-38.pyc  \n",
            "   creating: recsys/memories/\n",
            "  inflating: recsys/memories/ItemToItem.py  \n",
            "  inflating: recsys/memories/UserToUser.py  \n",
            "   creating: recsys/memories/__pycache__/\n",
            "  inflating: recsys/memories/__pycache__/UserToUser.cpython-36.pyc  \n",
            "  inflating: recsys/memories/__pycache__/UserToUser.cpython-37.pyc  \n",
            "  inflating: recsys/memories/__pycache__/ItemToItem.cpython-37.pyc  \n",
            "  inflating: recsys/memories/__pycache__/user2user.cpython-36.pyc  \n",
            "  inflating: recsys/memories/__pycache__/ItemToItem.cpython-36.pyc  \n",
            "   creating: recsys/models/\n",
            "  inflating: recsys/models/SVD.py    \n",
            "  inflating: recsys/models/MatrixFactorization.py  \n",
            "  inflating: recsys/models/ExplainableMF.py  \n",
            "  inflating: recsys/models/NonnegativeMF.py  \n",
            "   creating: recsys/models/__pycache__/\n",
            "  inflating: recsys/models/__pycache__/SVD.cpython-36.pyc  \n",
            "  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-37.pyc  \n",
            "  inflating: recsys/models/__pycache__/ExplainableMF.cpython-36.pyc  \n",
            "  inflating: recsys/models/__pycache__/ExplainableMF.cpython-37.pyc  \n",
            "  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-36.pyc  \n",
            "   creating: recsys/metrics/\n",
            "  inflating: recsys/metrics/EvaluationMetrics.py  \n",
            "   creating: recsys/img/\n",
            "  inflating: recsys/img/MF-and-NNMF.png  \n",
            "  inflating: recsys/img/svd.png      \n",
            "  inflating: recsys/img/MF.png       \n",
            "   creating: recsys/predictions/\n",
            "   creating: recsys/predictions/item2item/\n",
            "   creating: recsys/weights/\n",
            "   creating: recsys/weights/item2item/\n",
            "   creating: recsys/weights/item2item/ml1m/\n",
            "  inflating: recsys/weights/item2item/ml1m/similarities.npy  \n",
            "  inflating: recsys/weights/item2item/ml1m/neighbors.npy  \n",
            "   creating: recsys/weights/item2item/ml100k/\n",
            "  inflating: recsys/weights/item2item/ml100k/similarities.npy  \n",
            "  inflating: recsys/weights/item2item/ml100k/neighbors.npy  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not (os.path.exists(\"recsys.zip\") or os.path.exists(\"recsys\")):\n",
        "    !wget https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip    \n",
        "    !unzip recsys.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TDUsDpCqzWW"
      },
      "source": [
        "requirements\n",
        "\n",
        "matplotlib==3.2.2\n",
        "numpy==1.18.1\n",
        "pandas==1.0.5\n",
        "python==3.6.10\n",
        "scikit-learn==0.23.1\n",
        "scipy==1.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZJlYlbgq43k"
      },
      "outputs": [],
      "source": [
        "#Import packages\n",
        "from recsys.memories.UserToUser import UserToUser\n",
        "\n",
        "from recsys.preprocessing import mean_ratings\n",
        "from recsys.preprocessing import normalized_ratings\n",
        "from recsys.preprocessing import ids_encoder\n",
        "from recsys.preprocessing import train_test_split\n",
        "from recsys.preprocessing import rating_matrix\n",
        "from recsys.preprocessing import get_examples\n",
        "\n",
        "from recsys.datasets import ml100k, ml1m\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJMIc2yoq--0"
      },
      "source": [
        "Compute Explainable Scores\n",
        "\n",
        "Explainable score are computed using neighborhood based similarities. Here, we are using the user based algorithme to compute similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMpdIwVJrAiE"
      },
      "outputs": [],
      "source": [
        "#Compute Explainable Scores\n",
        "def explainable_score(user2user, users, items, theta=0):\n",
        "    \n",
        "    def _progress(count):\n",
        "        sys.stdout.write('\\rCompute Explainable score. Progress status : %.1f%%'%(float(count/len(users))*100.0))\n",
        "        sys.stdout.flush()\n",
        "    # initialize explainable score to zeros\n",
        "    W = np.zeros((len(users), len(items)))\n",
        "\n",
        "    for count, u in enumerate(users):            \n",
        "        candidate_items = user2user.find_user_candidate_items(u)        \n",
        "        for i in candidate_items:                \n",
        "            user_who_rated_i, similar_user_who_rated_i = \\\n",
        "                user2user.similar_users_who_rated_this_item(u, i)\n",
        "            if user_who_rated_i.shape[0] == 0:\n",
        "                w = 0.0\n",
        "            else:\n",
        "                w = similar_user_who_rated_i.shape[0] / user_who_rated_i.shape[0]\n",
        "            W[u,i] =  w  if w > theta else 0.0\n",
        "        _progress(count)\n",
        "    return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzlIfuUOrG1d"
      },
      "source": [
        "Explainable Matrix Factorization Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19xZ2o80rHpF"
      },
      "outputs": [],
      "source": [
        "#Explainable Matrix Factorization Model\n",
        "class ExplainableMatrixFactorization:\n",
        "    \n",
        "    def __init__(self, m, n, W, alpha=0.001, beta=0.01, lamb=0.1, k=10):\n",
        "        \"\"\"\n",
        "            - R : Rating matrix of shape (m,n) \n",
        "            - W : Explainability Weights of shape (m,n)\n",
        "            - k : number of latent factors\n",
        "            - beta : L2 regularization parameter\n",
        "            - lamb : explainability regularization coefficient\n",
        "            - theta : threshold above which an item is explainable for a user\n",
        "        \"\"\"\n",
        "        self.W = W\n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        \n",
        "        np.random.seed(64)\n",
        "        \n",
        "        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt\n",
        "        self.k = k\n",
        "        self.P = np.random.normal(size=(self.m,k))\n",
        "        self.Q = np.random.normal(size=(self.n,k))\n",
        "        \n",
        "        # hyperparameter initialization\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.lamb = lamb\n",
        "        \n",
        "        # training history\n",
        "        self.history = {\n",
        "            \"epochs\":[],\n",
        "            \"loss\":[],\n",
        "            \"val_loss\":[],\n",
        "        }\n",
        "        \n",
        "    def print_training_parameters(self):\n",
        "        print('Training EMF')\n",
        "        print(f'k={self.k} \\t alpha={self.alpha} \\t beta={self.beta} \\t lambda={self.lamb}')\n",
        "        \n",
        "    def update_rule(self, u, i, error):\n",
        "        self.P[u] = self.P[u] + \\\n",
        "            self.alpha*(2 * error*self.Q[i] - self.beta*self.P[u] - self.lamb*(self.P[u] - self.Q[i]) * self.W[u,i])\n",
        "        \n",
        "        self.Q[i] = self.Q[i] + \\\n",
        "            self.alpha*(2 * error*self.P[u] - self.beta*self.Q[i] + self.lamb*(self.P[u] - self.Q[i]) * self.W[u,i])\n",
        "        \n",
        "    def mae(self,  x_train, y_train):\n",
        "        \"\"\"\n",
        "        returns the Mean Absolute Error\n",
        "        \"\"\"\n",
        "        # number of training exemples\n",
        "        M = x_train.shape[0]\n",
        "        error = 0\n",
        "        for pair, r in zip(x_train, y_train):\n",
        "            u, i = pair\n",
        "            error += np.absolute(r - np.dot(self.P[u], self.Q[i]))\n",
        "        return error/M\n",
        "    \n",
        "    def print_training_progress(self, epoch, epochs, error, val_error, steps=5):\n",
        "        if epoch == 1 or epoch % steps == 0 :\n",
        "                print(f\"epoch {epoch}/{epochs} - loss : {round(error,3)} - val_loss : {round(val_error,3)}\")\n",
        "                \n",
        "    def learning_rate_schedule(self, epoch, target_epochs = 20):\n",
        "        if (epoch >= target_epochs) and (epoch % target_epochs == 0):\n",
        "                factor = epoch // target_epochs\n",
        "                self.alpha = self.alpha * (1 / (factor * 20))\n",
        "                print(\"\\nLearning Rate : {}\\n\".format(self.alpha))\n",
        "        \n",
        "    def fit(self, x_train, y_train, validation_data, epochs=10):\n",
        "        \"\"\"\n",
        "        Train latent factors P and Q according to the training set\n",
        "        \n",
        "        :param\n",
        "            - x_train : training pairs (u,i) for which rating r_ui is known\n",
        "            - y_train : set of ratings r_ui for all training pairs (u,i)\n",
        "            - validation_data : tuple (x_test, y_test)\n",
        "            - epochs : number of time to loop over the entire training set. \n",
        "            10 epochs by default\n",
        "            \n",
        "        Note that u and i are encoded values of userid and itemid\n",
        "        \"\"\"\n",
        "        self.print_training_parameters()\n",
        "        \n",
        "        # get validation data\n",
        "        x_test, y_test = validation_data\n",
        "        \n",
        "        for epoch in range(1, epochs+1):\n",
        "            for pair, r in zip(x_train, y_train):                \n",
        "                u,i = pair                \n",
        "                r_hat = np.dot(self.P[u], self.Q[i])                \n",
        "                e = r - r_hat\n",
        "                self.update_rule(u, i, error=e)\n",
        "                \n",
        "            # training and validation error  after this epochs\n",
        "            error = self.mae(x_train, y_train)\n",
        "            val_error = self.mae(x_test, y_test)\n",
        "            self.update_history(epoch, error, val_error)            \n",
        "            self.print_training_progress(epoch, epochs, error, val_error, steps=1)\n",
        "        \n",
        "        return self.history\n",
        "    \n",
        "    def update_history(self, epoch, error, val_error):\n",
        "        self.history['epochs'].append(epoch)\n",
        "        self.history['loss'].append(error)\n",
        "        self.history['val_loss'].append(val_error)\n",
        "    \n",
        "    def evaluate(self, x_test, y_test):\n",
        "        \"\"\"\n",
        "        compute the global error on the test set\n",
        "        \n",
        "        :param\n",
        "            - x_test : test pairs (u,i) for which rating r_ui is known\n",
        "            - y_test : set of ratings r_ui for all test pairs (u,i)\n",
        "        \"\"\"\n",
        "        error = self.mae(x_test, y_test)\n",
        "        print(f\"validation error : {round(error,3)}\")\n",
        "      \n",
        "    def predict(self, userid, itemid):\n",
        "        \"\"\"\n",
        "        Make rating prediction for a user on an item\n",
        "\n",
        "        :param\n",
        "        - userid\n",
        "        - itemid\n",
        "\n",
        "        :return\n",
        "        - r : predicted rating\n",
        "        \"\"\"\n",
        "        # encode user and item ids to be able to access their latent factors in\n",
        "        # matrices P and Q\n",
        "        u = uencoder.transform([userid])[0]\n",
        "        i = iencoder.transform([itemid])[0]\n",
        "\n",
        "        # rating prediction using encoded ids. Dot product between P_u and Q_i\n",
        "        r = np.dot(self.P[u], self.Q[i])\n",
        "\n",
        "        return r\n",
        "\n",
        "    def recommend(self, userid, N=30):\n",
        "        \"\"\"\n",
        "        make to N recommendations for a given user\n",
        "\n",
        "        :return \n",
        "        - (top_items,preds) : top N items with the highest predictions \n",
        "        \"\"\"\n",
        "        # encode the userid\n",
        "        u = uencoder.transform([userid])[0]\n",
        "\n",
        "        # predictions for this user on all product\n",
        "        predictions = np.dot(self.P[u], self.Q.T)\n",
        "\n",
        "        # get the indices of the top N predictions\n",
        "        top_idx = np.flip(np.argsort(predictions))[:N]\n",
        "\n",
        "        # decode indices to get their corresponding itemids\n",
        "        top_items = iencoder.inverse_transform(top_idx)\n",
        "\n",
        "        # take corresponding predictions for top N indices\n",
        "        preds = predictions[top_idx]\n",
        "\n",
        "        return top_items, preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-WQ5ksnrONO"
      },
      "outputs": [],
      "source": [
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbH17uPsrQx_"
      },
      "source": [
        "Model Evaluation\n",
        "1. MovieLens 100K\n",
        "1.1. Evaluation on raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd6vDVlErSAb",
        "outputId": "20549b8b-1c2a-486c-d40f-d23c6e40cc36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download data 100.2%\n",
            "Successfully downloaded ml-100k.zip 4924029 bytes.\n",
            "Unzipping the ml-100k.zip zip file ...\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "ratings, movies = ml100k.load()\n",
        "\n",
        "# encode users and items ids\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "users = sorted(ratings.userid.unique())\n",
        "items = sorted(ratings.itemid.unique())\n",
        "\n",
        "m = len(users)\n",
        "n = len(items)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftbQB54HrX2c",
        "outputId": "c11cfee3-acee-47f9-9a08-14da0b042fb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalize users ratings ...\n",
            "Initialize the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "User to user recommendation model created with success ...\n",
            "Compute Explainable score. Progress status : 99.9%"
          ]
        }
      ],
      "source": [
        "# create the user to user model for similarity measure\n",
        "usertouser = UserToUser(ratings, movies)\n",
        "\n",
        "# compute explainable score\n",
        "W = explainable_score(usertouser, users, items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjew6pQvrabj",
        "outputId": "4589a85d-48a6-4b6e-d10e-610104cae993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EMF\n",
            "k=10 \t alpha=0.01 \t beta=0.4 \t lambda=0.01\n",
            "epoch 1/10 - loss : 0.922 - val_loss : 1.036\n",
            "epoch 2/10 - loss : 0.79 - val_loss : 0.873\n",
            "epoch 3/10 - loss : 0.766 - val_loss : 0.837\n",
            "epoch 4/10 - loss : 0.757 - val_loss : 0.822\n",
            "epoch 5/10 - loss : 0.753 - val_loss : 0.814\n",
            "epoch 6/10 - loss : 0.751 - val_loss : 0.808\n",
            "epoch 7/10 - loss : 0.749 - val_loss : 0.805\n",
            "epoch 8/10 - loss : 0.748 - val_loss : 0.802\n",
            "epoch 9/10 - loss : 0.746 - val_loss : 0.799\n",
            "epoch 10/10 - loss : 0.745 - val_loss : 0.797\n"
          ]
        }
      ],
      "source": [
        "# initialize the model\n",
        "EMF = ExplainableMatrixFactorization(m, n, W, alpha=0.01, beta=0.4, lamb=0.01, k=10)\n",
        "\n",
        "history = EMF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urIWKDzKrco4",
        "outputId": "d4a897f9-2e06-4b2a-883b-f077033736f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation error : 0.797\n"
          ]
        }
      ],
      "source": [
        "EMF.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njJAkA85rdh_"
      },
      "source": [
        "1.2. Evaluation on normalized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acCXn8oFrfPl"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "ratings, movies = ml100k.load()\n",
        "\n",
        "# encode users and items ids\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "users = sorted(ratings.userid.unique())\n",
        "items = sorted(ratings.itemid.unique())\n",
        "\n",
        "m = len(users)\n",
        "n = len(items)\n",
        "\n",
        "# normalize ratings by substracting means\n",
        "normalized_column_name = \"norm_rating\"\n",
        "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoA1lGjprnYo",
        "outputId": "57c5eb82-5aa8-4390-9fde-95f5c51fc55a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EMF\n",
            "k=10 \t alpha=0.022 \t beta=0.65 \t lambda=0.01\n",
            "epoch 1/10 - loss : 0.809 - val_loss : 0.842\n",
            "epoch 2/10 - loss : 0.809 - val_loss : 0.829\n",
            "epoch 3/10 - loss : 0.807 - val_loss : 0.821\n",
            "epoch 4/10 - loss : 0.799 - val_loss : 0.811\n",
            "epoch 5/10 - loss : 0.789 - val_loss : 0.8\n",
            "epoch 6/10 - loss : 0.782 - val_loss : 0.793\n",
            "epoch 7/10 - loss : 0.778 - val_loss : 0.789\n",
            "epoch 8/10 - loss : 0.776 - val_loss : 0.786\n",
            "epoch 9/10 - loss : 0.774 - val_loss : 0.784\n",
            "epoch 10/10 - loss : 0.773 - val_loss : 0.783\n"
          ]
        }
      ],
      "source": [
        "# initialize the model\n",
        "EMF = ExplainableMatrixFactorization(m, n, W, alpha=0.022, beta=0.65, lamb=0.01, k=10)\n",
        "\n",
        "history = EMF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrzKVfXnrqNi"
      },
      "source": [
        "2. MovieLens 1M\n",
        "2.1.Evaluation on raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSXOptYirrcT",
        "outputId": "7d509a33-7b68-46d1-96d4-2fea897d385a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download data 100.1%\n",
            "Successfully downloaded ml-1m.zip 5917549 bytes.\n",
            "Unzipping the ml-1m.zip zip file ...\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "ratings, movies = ml1m.load()\n",
        "\n",
        "# encode users and items ids\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "users = sorted(ratings.userid.unique())\n",
        "items = sorted(ratings.itemid.unique())\n",
        "\n",
        "m = len(users)\n",
        "n = len(items)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5XZlGJ4r1PF",
        "outputId": "132833d5-55b8-4ad3-fb17-712b95ec47ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalize users ratings ...\n",
            "Initialize the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "User to user recommendation model created with success ...\n",
            "Compute Explainable score. Progress status : 100.0%"
          ]
        }
      ],
      "source": [
        "# create the user to user model for similarity measure\n",
        "usertouser = UserToUser(ratings, movies)\n",
        "\n",
        "# compute explainable score\n",
        "W = explainable_score(usertouser, users, items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "299GZGo7r3jS",
        "outputId": "46b3ebf7-1ff6-4ce9-f31c-bba7f8123feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training EMF\n",
            "k=10 \t alpha=0.01 \t beta=0.4 \t lambda=0.01\n",
            "epoch 1/10 - loss : 0.782 - val_loss : 0.807\n",
            "epoch 2/10 - loss : 0.762 - val_loss : 0.781\n",
            "epoch 3/10 - loss : 0.76 - val_loss : 0.775\n",
            "epoch 4/10 - loss : 0.758 - val_loss : 0.771\n",
            "epoch 5/10 - loss : 0.757 - val_loss : 0.769\n",
            "epoch 6/10 - loss : 0.756 - val_loss : 0.767\n",
            "epoch 7/10 - loss : 0.754 - val_loss : 0.764\n",
            "epoch 8/10 - loss : 0.752 - val_loss : 0.762\n",
            "epoch 9/10 - loss : 0.751 - val_loss : 0.761\n",
            "epoch 10/10 - loss : 0.75 - val_loss : 0.76\n"
          ]
        }
      ],
      "source": [
        "# initialize the model\n",
        "EMF = ExplainableMatrixFactorization(m, n, W, alpha=0.01, beta=0.4, lamb=0.01, k=10)\n",
        "\n",
        "history = EMF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz6WgnhPr5Op"
      },
      "source": [
        "2.2. Evaluation on normalized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzsW-Mqwr6LA"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "ratings, movies = ml1m.load()\n",
        "\n",
        "# encode users and items ids\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "# normalize ratings by substracting means\n",
        "normalized_column_name = \"norm_rating\"\n",
        "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eX7KZebr_7V"
      },
      "outputs": [],
      "source": [
        "# initialize the model\n",
        "EMF = ExplainableMatrixFactorization(m, n, W, alpha=0.023, beta=0.59, lamb=0.01, k=10)\n",
        "\n",
        "history = EMF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV37thYvsBom"
      },
      "source": [
        "Ratings prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLXPdciLsEJu"
      },
      "outputs": [],
      "source": [
        "# get list of top N items with their corresponding predicted ratings\n",
        "userid = 42\n",
        "recommended_items, predictions = EMF.recommend(userid=userid)\n",
        "\n",
        "# find corresponding movie titles\n",
        "top_N = list(zip(recommended_items,predictions))\n",
        "top_N = pd.DataFrame(top_N, columns=['itemid','predictions'])\n",
        "top_N.predictions = top_N.predictions + ratings.loc[ratings.userid==userid].rating_mean.values[0]\n",
        "List = pd.merge(top_N, movies, on='itemid', how='inner')\n",
        "\n",
        "# show the list\n",
        "List"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5Dlvd81nWuah1O+p99Ss1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}